# -*- coding: utf-8 -*-
"""EN3-BT MCD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cnvSgNDexJ0cqrTWGygI_smZ0y8EIWZn
"""

import torch
import numpy as np
import tqdm
import copy
from torch.nn import functional as F
from torch.nn.modules.module import Module
from sklearn.calibration import calibration_curve, CalibratedClassifierCV
from torch.nn.modules.activation import MultiheadAttention
from torch.nn.modules.container import ModuleList
from torch.nn.init import xavier_uniform_
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.linear import Linear
from torch.nn.modules.normalization import LayerNorm
from torch.utils.data import DataLoader, Dataset
import logging
logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')
logging.getLogger().setLevel(logging.INFO)

from utils import *
import math
from torch.autograd import Variable
import re
import pandas as pd
torch.manual_seed(2)

def set_dropout_to_train(m):
    if type(m) == torch.nn.Dropout:
        m.train()

class Embedder(Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        print(vocab_size, d_model)
        self.embed = torch.nn.Embedding(vocab_size + 1, d_model)
        
    def forward(self, x):
        x = self.embed(x)
        return x

class PositionalEncoder(Module):
    def __init__(self, d_model, max_seq_len = 768, dropout = 0.5):
        super().__init__()
        self.d_model = d_model
        self.dropout = Dropout(dropout)
        # create constant 'pe' matrix with values dependant on 
        # pos and i
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i)/d_model)))
                pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
 
    
    def forward(self, x):
        # make embeddings relatively larger
        x = x * math.sqrt(self.d_model)
        #add constant to embedding
        seq_len = x.size(1)
        pe = Variable(self.pe[:,:seq_len], requires_grad=False)
        if x.is_cuda:
            pe.cuda()
        x = x + pe
        return self.dropout(x)

def get_clones(module, N):
    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])

def attention(q, k, v, d_k, mask=None, dropout=None):
    
    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)
    scores = F.softmax(scores, dim=-1)
    
    if dropout is not None:
        scores = dropout(scores)
        
    output = torch.matmul(scores, v)
    return output

class MultiHeadAttention(Module):
    def __init__(self, heads, d_model, dropout = 0.5):
        super().__init__()
        
        self.d_model = d_model
        self.d_k = d_model // heads
        self.h = heads
        
        self.q_linear = torch.nn.Linear(d_model, d_model)
        self.v_linear = torch.nn.Linear(d_model, d_model)
        self.k_linear = torch.nn.Linear(d_model, d_model)
        self.dropout = Dropout(dropout)
        self.out = torch.nn.Linear(d_model, d_model)
    
    def forward(self, q, k, v, mask=None):
        
        bs = q.size(0)
        
        # perform linear operation and split into h heads
        
        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)
        
        # transpose to get dimensions bs * h * sl * d_model
       
        k = k.transpose(1,2)
        q = q.transpose(1,2)
        v = v.transpose(1,2)
        # calculate attention using function we will define next
        scores = attention(q, k, v, self.d_k, mask, self.dropout)
        
        # concatenate heads and put through final linear layer
        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)
        
        output = self.out(concat)
    
        return output

class Norm(Module):
    def __init__(self, d_model, eps = 1e-6):
        super().__init__()
    
        self.size = d_model
        # create two learnable parameters to calibrate normalisation
        self.alpha = torch.nn.Parameter(torch.ones(self.size))
        self.bias = torch.nn.Parameter(torch.zeros(self.size))
        self.eps = eps
    def forward(self, x):
        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \
        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias
        return norm

class FeedForward(Module):
    def __init__(self, d_model, d_ff=2048, dropout = 0.3):
        super().__init__() 
        # We set d_ff as a default to 2048
        self.linear_1 = torch.nn.Linear(d_model, d_ff)
        self.dropout = Dropout(dropout)
        self.linear_2 = torch.nn.Linear(d_ff, d_model)
        
    def forward(self, x):
        x = self.dropout(F.relu(self.linear_1(x)))
        x = self.linear_2(x)
        return x

class EncoderLayer(Module):
    def __init__(self, d_model, heads, dropout = 0.3):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.attn = MultiHeadAttention(heads, d_model)
        self.ff = FeedForward(d_model)
        self.dropout_1 = Dropout(dropout)
        self.dropout_2 = Dropout(dropout)
        
    def forward(self, x, mask = None):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn(x2,x2,x2,mask = None))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.ff(x2))
        return x

class Encoder(Module):
    def __init__(self, vocab_size = 1000, d_model = 32, N = 1, heads= 1):
        super().__init__()
        self.N = N
        self.embed = Embedder(vocab_size, d_model)
        self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(EncoderLayer(d_model, heads), N)
        self.norm = Norm(d_model)
        self.output_layer = torch.nn.Linear(d_model**2, 1)
        self.output_activation = torch.nn.Sigmoid()
        
    def forward(self, src):

        bdim = src.shape[0]
        x = self.embed(src)

        #tlen = int(math.sqrt(src.shape[1]))
        #x = src.reshape(int(src.shape[0]), tlen, tlen)
        
        x = self.pe(x)
        
        for i in range(self.N):
            x = self.layers[i](x)
            
        x = self.norm(x)
        outputs = torch.autograd.Variable(torch.zeros(bdim), requires_grad = False)

        for j in range(bdim):
            outputs[j] = self.output_layer(x[j,:,:].flatten())
        
        s = self.output_activation(outputs)
        
        return s

class Dataset_single(Dataset):
    def __init__(self, features, targets = None, transform=None):
        self.features = features

        if not targets is None:
            self.targets = np.array(targets)
        else:
            self.targets = None

    def __len__(self):
        return self.features.shape[0]

    def __getitem__(self, index):
        instance = torch.tensor(self.features[index], 
                                dtype=torch.long, 
                                device='cpu')
        if self.targets is not None:
            target = torch.as_tensor(self.targets.reshape(-1, 1)[index],
                                     device='cpu')
        else:
            target = -1
        return instance, target


# This is were we train our model
class BAN:

    def __init__ (self, num_epochs = 200, vocab_size = 100000, stopping_crit = 5, learning_rate = 0.001, tokenizer_num_words = 100000, max_padding = 256,N=1,heads = 1, batch_size = 64):

        #self.learning_rate = 0.001
        
        self.d_model = max_padding
        self.N = N
        self.attention_heads = heads
        self.max_padding = max_padding
        self.image_folder = None
        self.learning_rate = learning_rate
        self.classes_ = [0,1]
        self.validation_index = 0
        self.batch_size = batch_size
        self.threshold_perf_tuples = []
        self.num_epochs = num_epochs
        self.probability_threshold = None
        self.vocab_size = vocab_size
        self.stopping_crit = stopping_crit
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logging.info("Using {}".format(self.device))
        self.tokenizer = Tokenizer(num_words=tokenizer_num_words)

    def pad_sequence(self, list_of_texts):
        # https://towardsdatascience.com/text-classification-in-keras-part-2-how-to-use-the-keras-tokenizer-word-representations-fd571674df23
        
       # pad_seq = get_input_attentions(list_of_texts, max_size = self.max_padding)
       # logging.info(pad_seq.shape)
                    
        self.tokenizer.fit_on_texts(list_of_texts)
        sequences = self.tokenizer.texts_to_sequences(list_of_texts)
        pad_seq = pad_sequences(sequences, maxlen=self.max_padding)
        
        return pad_seq

    def encode_input_text_integer(self, list_of_texts, mapping = None):

        ## somewhat adhoc -> can be improved -> TODO: Byte pair.        
        unique_words = set()
        for text in list_of_texts:
            [unique_words.add(x) for x in text.strip().split()]
        unique_words = list(unique_words)

        if mapping is None:
            mapping = {}
            for index, word in enumerate(unique_words):
                mapping[word] = index+1
                
        encoded_texts = []
        for text in list_of_texts:
            encoded_sentence = [mapping[x] for x in text.strip().split()] + [0]*self.max_padding
            encoded_texts.append(np.array(encoded_sentence[0:self.max_padding]))
        return encoded_texts, mapping


    def predict_proba(self, input_text_sequences, T=10, output_mean_probabilities = True):


        pad_seq = input_text_sequences #pad_sequences(input_text_sequences, maxlen=self.max_padding)
        pad_seq = np.array(pad_seq)
        
        val_dataset = Dataset_single(pad_seq)
        val_dataset = DataLoader(val_dataset, batch_size = self.batch_size, shuffle = False)
        
        outputs = []
        p = []
        w= []

        for i, (features, labels) in tqdm.tqdm(enumerate(val_dataset), total = len(val_dataset)):
            features = features.to(self.device)
            collection_of_preds = []
            
            self.model.eval() ## Immutable during predictions.
            self.model.apply(set_dropout_to_train) ## Unlock Dropout layers.
            
            for _ in range(T):
                prediction = self.model(features).detach().cpu().numpy()
                collection_of_preds.append(prediction)

            p = np.matrix(collection_of_preds).T
            w.append(p)

        w = np.concatenate(w, axis = 0)
        
        assert w.shape[0] == pad_seq.shape[0]        
        assert w.shape[1] == T

        MC_pred = w.reshape(pad_seq.shape[0], T)
        pred = pd.DataFrame(MC_pred)
        MC_pred_positive = pred.mean(axis=1).values
        MC_pred_negative = 1 - MC_pred_positive
        MC_pred = np.vstack((MC_pred_negative, MC_pred_positive)).T

        self.model.train()
        return MC_pred.astype(np.float64)
    
    def predict(self, input_text_sequences, T=100, output_mean_probabilities = True):


        pad_seq = input_text_sequences #pad_sequences(input_text_sequences, maxlen=self.max_padding)
        pad_seq = np.array(pad_seq)
        
        val_dataset = Dataset_single(pad_seq)
        val_dataset = DataLoader(val_dataset, batch_size = self.batch_size, shuffle = False)
        
        outputs = []
        p = []
        w= []

        for i, (features, labels) in tqdm.tqdm(enumerate(val_dataset), total = len(val_dataset)):
            features = features.to(self.device)
            collection_of_preds = []
            
            self.model.eval() ## Immutable during predictions.
            self.model.apply(set_dropout_to_train) ## Unlock Dropout layers.
            
            for _ in range(T):
                prediction = self.model(features).detach().cpu().numpy()
                collection_of_preds.append(prediction)

            p = np.matrix(collection_of_preds).T
            w.append(p)

        w = np.concatenate(w, axis = 0)
        
        assert w.shape[0] == pad_seq.shape[0]
        assert w.shape[1] == T

        if output_mean_probabilities:
            MC_pred = w.reshape(pad_seq.shape[0], T)
            pred = pd.DataFrame(MC_pred)
            MC_pred = pred.mean(axis=1).values
            
        else:
            MC_pred = w.reshape(pad_seq.shape[0], T)
            
        self.model.train()
        return MC_pred

    def ece_score(self, probab_pred, real_y, mbin = 3, threshold = 0.5):

        all_vals = len(real_y)
        bin_perf = []
        current_bin = 0

        predictions = probab_pred.copy()
        predictions[predictions >= threshold] = 1
        predictions[predictions < threshold] = 0

        reals_internal = []            
        predictions_internal = []

        ## compute bins (last one is extended with the remainder)
        intercept_bins = [x for x in range(1,all_vals) if x % mbin == 0]
        remainder = all_vals % mbin
        if len(intercept_bins) == 0:
            intercept_bins = [all_vals]
        intercept_bins[-1] += remainder

        intercept_index = 0
        for j in range(all_vals):

            if j == intercept_bins[intercept_index] and j > 0:

                if intercept_index < len(intercept_bins)-1:
                    intercept_index += 1

                current_bin += 1
                equals = np.where(np.array(reals_internal) == np.array(predictions_internal))
                acc_bin = len(equals)/len(predictions_internal)

                conf_bin = np.mean(np.array(predictions_internal))
                bin_perf.append([current_bin, acc_bin, conf_bin,len(reals_internal)])

                reals_internal = [real_y[j]]
                predictions_internal = [predictions[j]]

            else:
                reals_internal.append(real_y[j])
                predictions_internal.append(predictions[j])

        ece_score_final = 0
        for bins in bin_perf:
            bin_size = bins[3]
            total = len(probab_pred)
            partial = (bin_size/total) * np.abs(bins[1] - bins[2])
            ece_score_final += partial

        return ece_score_final
                
    
    def fit(self, input_text_sequences, targets, val_percentage = 0.2, adaptive_threshold = True, validation_metric = "precision"):
        """
        The main fit method. Given an ordered set of documents, this train the architecture 
        along with intermediary, validation set-based calibration. The validation
        percentage is specified with
        :param input_text_sequences: inputs
        :param targets: target vector
        :val_percentage: percentage used for stopping + calibration assessment
        """

        ## generate stratified split for validation
        already_traversed = set()
        total_val = int(val_percentage * len(targets))
        validation_indices = []
        training_indices = []
        trigger = False
        
        vnum = int(input_text_sequences.shape[0]*val_percentage)
        input_text_sequences = input_text_sequences[vnum:]
        targets = targets[vnum:]
        self.validation_index = vnum
        
        ## get val data
        val_sequences = input_text_sequences[:vnum]
        val_targets = targets[:vnum]
        
        train_dataset = Dataset_single(input_text_sequences, targets)
        train_dataset = DataLoader(train_dataset, batch_size = self.batch_size, shuffle = True)

        val_dataset = Dataset_single(val_sequences, val_targets)
        val_dataset = DataLoader(val_dataset, batch_size = 1, shuffle = False)
        self.validation_loader = val_dataset ## this is used for temperature-based calibration
        
        self.loss = torch.nn.BCELoss()        
        self.model = Encoder(vocab_size = self.vocab_size, d_model = self.d_model, N = self.N, heads= self.attention_heads)
        self.model.train()
        self.optimizer = torch.optim.Adamax(self.model.parameters(), lr=self.learning_rate)
        self.num_params = sum(p.numel() for p in self.model.parameters())
        logging.info("Number of parameters {}".format(self.num_params))
        
       # for param_tensor in self.model.state_dict():
       #     logging.info(" ".join(str(x) for x in [param_tensor, "\t", self.model.state_dict()[param_tensor].size()]))
            
        current_loss = 0
        loss = 1
        stopping_iteration = 0
        amax = 0
        stopping = 0
        top_state_dict = None
        g_amax = 0

        for epoch in range(self.num_epochs):
            
            if stopping >= self.stopping_crit:
                logging.info("Stopping ..")
                break
            
            # here we put all the losses 
            losses_per_batch = []
            self.model.train()
            for i, (features, labels) in tqdm.tqdm(enumerate(train_dataset), total = len(train_dataset)):
                
                # defining the input
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                self.model.to(self.device)
                
                outputs = self.model(features)

                ## if unable to predict predict random.
                loss = self.loss(outputs, labels.view(-1).cpu().float())

                self.optimizer.zero_grad()
                
                loss.backward()
                
                self.optimizer.step()
                
                losses_per_batch.append(float(loss))

            means_pred = self.predict(val_sequences, T = 30)
            
            assert len(means_pred) == len(val_targets)
            val_acc = 0

            if adaptive_threshold:
                for threshold in np.arange(0.1,0.9,0.0005):

                    ## Copy not to overwrite
                    means = means_pred.copy()
                    means[means >= threshold] = 1
                    means[means < threshold] = 0

                    if validation_metric == "accuracy":
                        acc = metrics.accuracy_score(val_targets, means)
                        
                    else:
                        acc = metrics.precision_score(val_targets, means) *  metrics.accuracy_score(val_targets, means)


                    if val_acc < acc:
                        val_acc = acc

                    if acc > amax:
                        amax = acc
                        self.probability_threshold = threshold
                        self.threshold_perf_tuples.append([amax, threshold])
                        top_state_dict = self.model.state_dict()
                        logging.info("New top Score: {}, thr: {}".format(amax, threshold))
            else:
                
                threshold = 0.5
                means = means_pred.copy()
                means[means >= threshold] = 1
                means[means < threshold] = 0               
                acc = metrics.accuracy_score(val_targets, means)

                if val_acc < acc:
                    val_acc = acc

                if acc > amax:
                    amax = acc
                    self.probability_threshold = threshold
                    self.threshold_perf_tuples.append([amax, threshold])
                    top_state_dict = self.model.state_dict()
                    logging.info("New top Acc: {}, thr: {}".format(amax, threshold))                   
                    
            if  amax > val_acc:
                stopping += 1
                    
            mean_loss = np.mean(losses_per_batch)
            logging.info("epoch {}, mean loss per batch {}, threshold: {}, MaxScore: {}".format(epoch, mean_loss, np.round(self.probability_threshold,2), amax))

        ## revert to the top-performing parameter setting.
        self.model.load_state_dict(top_state_dict)

        fop, mpv = calibration_curve(val_targets, means_pred, n_bins=10)
        plt.plot([0, 1], [0, 1], linestyle='--', color = "black")
        plt.plot(mpv, fop, marker='.', color = "red")
        
        plt.xlabel("Mean prediction value")
        plt.ylabel("Fraction of positives")
        plt.savefig(self.image_folder+"/training_cal_{}_{}_visualization.pdf".format(self.num_epochs, adaptive_threshold), dpi = 300)
        plt.clf()
        
        # Return model
        return self.model

if __name__ == "__main__":

    # Read data from file
    import pandas as pd
    from sklearn.utils import shuffle
    from sklearn import metrics
    import numpy as np
    from sklearn.model_selection import StratifiedKFold
    import os

    from sklearn.datasets import fetch_20newsgroups
    newsgroups_train = fetch_20newsgroups(subset='train') ## some random data
    X = newsgroups_train['data']
    Y = newsgroups_train['target']
    print(len(X))
    skf = StratifiedKFold(n_splits=5)
    final_scores = []
    heads = 2
    max_padding = 200
    learning_rate = 0.01
    num_epochs = 300
    num_layers = 2
    for train_index, test_index in skf.split(X, Y):    
        nnet = BAN(heads = heads, max_padding = max_padding, learning_rate = learning_rate, num_epochs = num_epochs, batch_size = 8, N = num_layers, stopping_crit = 20)
        total_padded = nnet.pad_sequence(X)
        x_train = total_padded[train_index]
        x_test = total_padded[test_index]
        y_train = Y[train_index]
        y_test = Y[test_index]
        nnet.fit(x_train, y_train, adaptive_threshold = False, val_percentage = 0.1)
        predictions = nnet.predict(x_test)
        score = metrics.f1_score(predictions, y_test)
        final_scores.append(score)        
    mean_per = np.mean(final_scores)
    std_per = np.std(final_scores)
    
    print("Final performance (F1): {mean_per} +- {std_per}")
